
## result loggin
### top acc / good 

# param optimisation
## automatic testing
## define stuff before testing
##    - https://keras.io/callbacks/
##      - propper result logging
##      - earlyStopping -!!-
##          - loss? / val_acc? / val_loss?
##      - tensorboard -!!-
##          - param optimization
##    - learning log

# additional information
# https://www.linkedin.com/pulse/python-tutorial-bollinger-bands-andrew-hamlet
# https://www.linkedin.com/pulse/python-tutorial-macd-moving-average-andrew-hamlet

# regularisation
## kernel_regularizer? / bias_regularizer? / activity_regularizer?
# http://neuralnetworksanddeeplearning.com/chap3.html#overfitting_and_regularization

# results/<base lvl>/<1th level>/<2nd lvl>
# - csv / weights / tensorboard

# visual
## https://keras.io/visualization/

# Conv2D specifications (cumute / work)
## convolution window sizing

# classifier.fit optimisation

# general optimisation techniques
## https://cambridgespark.com/content/tutorials/neural-networks-tuning-techniques/index.html

# general overview
## http://neuralnetworksanddeeplearning.com/chap3.html#overfitting_and_regularization

# initialisation weights
## https://arxiv.org/pdf/1703.04691.pdf
## https://stackoverflow.com/questions/46798708/keras-how-to-view-initialized-weights-i-e-before-training

# leaning rate scheduling
# google search: keras learning rate decay
## https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/
## https://github.com/keras-team/keras/issues/898

# set categories with from sklearn.preprocessing import LabelEncoder, OneHotEncoder library
## help https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/

# course from marco
## https://www.coursera.org/learn/machine-learning
# go article
## https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ


